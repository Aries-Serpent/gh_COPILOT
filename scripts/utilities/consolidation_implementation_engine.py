#!/usr/bin/env python3
"""
Consolidation Implementation Engine
==================================

Purpose: Execute script consolidation based on modulation analysis results
Target: Implement Step 4 consolidation opportunities with enterprise safety

Features:
- Intelligent consolidation execution with backup protocols
- Enterprise-grade safety validation and rollback capability
- DUAL COPILOT pattern validation throughout consolidation process
- Database-driven consolidation tracking and validation
"""

import json
import shutil
import datetime
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass
from tqdm import tqdm

@dataclass
class ConsolidationExecution:
    """Represents a consolidation execution result"""
    opportunity_id: str
    primary_script: str
    consolidated_scripts: List[str]
    status: str
    lines_saved: int
    backup_location: str
    execution_time: float
    validation_results: Dict[str, Any]

class ConsolidationEngine:
    """ğŸ”§ Enterprise Script Consolidation Implementation Engine"""
    
    def __init__(self, workspace_path: str):
        self.workspace_path = Path(workspace_path)
        self.reports_dir = self.workspace_path / "reports"
        self.backup_root = Path("E:/temp/gh_COPILOT_Backups/consolidation_backups")
        self.archive_dir = self.workspace_path / "archive" / "consolidated_scripts"
        
        # CRITICAL: Ensure external backup location (anti-recursion)
        self.backup_root.mkdir(parents=True, exist_ok=True)
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        self.execution_results = {
            "timestamp": datetime.datetime.now().isoformat(),
            "workspace": str(self.workspace_path),
            "consolidations_executed": [],
            "total_lines_saved": 0,
            "execution_duration": 0.0,
            "success_rate": 0.0
        }
    
    def load_analysis_results(self) -> Dict[str, Any]:
        """Load the most recent modulation analysis results"""
        analysis_files = list(self.reports_dir.glob("script_modulation_analysis_*.json"))
        if not analysis_files:
            raise FileNotFoundError("No modulation analysis results found. Run script_modulation_analyzer.py first.")
        
        # Get the most recent analysis
        latest_analysis = max(analysis_files, key=lambda p: p.stat().st_mtime)
        
        with open(latest_analysis, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def validate_consolidation_safety(self, opportunity: Dict[str, Any]) -> bool:
        """CRITICAL: Validate consolidation safety before execution"""
        
        # Check if primary script exists and is readable
        primary_path = Path(opportunity["primary_script"])
        if not primary_path.exists():
            return False
        
        # Check if all similar scripts exist
        for script_path in opportunity["similar_scripts"]:
            if not Path(script_path).exists():
                return False
        
        # CRITICAL: Ensure no backup folder inside workspace
        for script_path in [opportunity["primary_script"]] + opportunity["similar_scripts"]:
            if "backup" in script_path.lower() and str(self.workspace_path) in script_path:
                print(f"âš ï¸  WARNING: Skipping potential recursive backup: {script_path}")
                return False
        
        return True
    
    def create_safe_backup(self, scripts: List[str], opportunity_id: str) -> str:
        """Create safe external backup of scripts before consolidation"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = self.backup_root / f"consolidation_{opportunity_id}_{timestamp}"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        for script_path in scripts:
            script = Path(script_path)
            if script.exists():
                backup_path = backup_dir / script.name
                shutil.copy2(script, backup_path)
        
        return str(backup_dir)
    
    def execute_high_priority_consolidations(self, max_consolidations: int = 10) -> List[ConsolidationExecution]:
        """Execute high-priority consolidations with maximum safety"""
        analysis_results = self.load_analysis_results()
        opportunities = analysis_results["consolidation_opportunities"]
        
        print("ğŸš€ STARTING HIGH-PRIORITY CONSOLIDATION EXECUTION")
        print("=" * 80)
        print(f"ğŸ“Š Total Opportunities Available: {len(opportunities)}")
        print(f"ğŸ¯ Target Executions: {min(max_consolidations, len(opportunities))}")
        print("=" * 80)
        
        executions = []
        
        # Sort by estimated savings (highest first) and filter safe opportunities
        safe_opportunities = []
        for i, opp in enumerate(opportunities[:max_consolidations]):
            if self.validate_consolidation_safety(opp):
                safe_opportunities.append((f"CONS_{i:03d}", opp))
            else:
                print(f"âš ï¸  Skipping unsafe consolidation: {Path(opp['primary_script']).name}")
        
        print(f"âœ… Validated {len(safe_opportunities)} safe consolidation opportunities")
        
        # Execute consolidations with progress tracking
        with tqdm(total=len(safe_opportunities), desc="ğŸ”§ Executing Consolidations", unit="consolidations") as pbar:
            for opportunity_id, opportunity in safe_opportunities:
                pbar.set_description(f"ğŸ”§ Consolidating {Path(opportunity['primary_script']).name}")
                
                execution = self.execute_single_consolidation(opportunity_id, opportunity)
                executions.append(execution)
                
                pbar.update(1)
                pbar.set_postfix({
                    'Status': execution.status,
                    'Lines Saved': execution.lines_saved
                })
        
        return executions
    
    def execute_single_consolidation(self, opportunity_id: str, opportunity: Dict[str, Any]) -> ConsolidationExecution:
        """Execute a single consolidation with full safety protocols"""
        start_time = datetime.datetime.now()
        
        primary_script = opportunity["primary_script"]
        similar_scripts = opportunity["similar_scripts"]
        all_scripts = [primary_script] + similar_scripts
        
        # Create backup
        backup_location = self.create_safe_backup(all_scripts, opportunity_id)
        
        try:
            # For this initial implementation, we'll perform a safe "archive" consolidation
            # rather than actual code merging (which requires more complex logic)
            
            # Move similar scripts to archive (keeping primary script)
            archived_scripts = []
            for script_path in similar_scripts:
                script = Path(script_path)
                if script.exists():
                    archive_path = self.archive_dir / script.name
                    
                    # Handle naming conflicts
                    counter = 1
                    while archive_path.exists():
                        stem = script.stem
                        suffix = script.suffix
                        archive_path = self.archive_dir / f"{stem}_{counter}{suffix}"
                        counter += 1
                    
                    shutil.move(str(script), str(archive_path))
                    archived_scripts.append(str(archive_path))
            
            # Calculate lines saved (estimated)
            lines_saved = opportunity.get("estimated_savings", 0)
            
            # Validation results
            validation_results = {
                "primary_script_exists": Path(primary_script).exists(),
                "scripts_archived": len(archived_scripts),
                "backup_created": Path(backup_location).exists(),
                "archive_integrity": all(Path(p).exists() for p in archived_scripts),
                "consolidation_method": "archive_duplicates"
            }
            
            execution_time = (datetime.datetime.now() - start_time).total_seconds()
            
            return ConsolidationExecution(
                opportunity_id=opportunity_id,
                primary_script=primary_script,
                consolidated_scripts=archived_scripts,
                status="SUCCESS",
                lines_saved=lines_saved,
                backup_location=backup_location,
                execution_time=execution_time,
                validation_results=validation_results
            )
            
        except Exception as e:
            # Rollback on error
            self.rollback_consolidation(backup_location, all_scripts)
            
            execution_time = (datetime.datetime.now() - start_time).total_seconds()
            
            return ConsolidationExecution(
                opportunity_id=opportunity_id,
                primary_script=primary_script,
                consolidated_scripts=[],
                status=f"FAILED: {str(e)}",
                lines_saved=0,
                backup_location=backup_location,
                execution_time=execution_time,
                validation_results={"error": str(e)}
            )
    
    def rollback_consolidation(self, backup_location: str, original_scripts: List[str]):
        """Rollback a consolidation by restoring from backup"""
        backup_dir = Path(backup_location)
        if not backup_dir.exists():
            return
        
        print(f"ğŸ”„ Rolling back consolidation from {backup_location}")
        
        for backup_file in backup_dir.glob("*.py"):
            # Find the original location
            for original_script in original_scripts:
                original_path = Path(original_script)
                if original_path.name == backup_file.name:
                    shutil.copy2(backup_file, original_path)
                    break
    
    def save_execution_results(self, executions: List[ConsolidationExecution]) -> str:
        """Save consolidation execution results"""
        
        # Calculate summary statistics
        successful_executions = [e for e in executions if e.status == "SUCCESS"]
        total_lines_saved = sum(e.lines_saved for e in successful_executions)
        success_rate = len(successful_executions) / len(executions) if executions else 0.0
        
        self.execution_results.update({
            "consolidations_executed": len(executions),
            "successful_consolidations": len(successful_executions),
            "total_lines_saved": total_lines_saved,
            "success_rate": success_rate,
            "execution_details": []
        })
        
        # Add execution details
        for execution in executions:
            self.execution_results["execution_details"].append({
                "opportunity_id": execution.opportunity_id,
                "primary_script": execution.primary_script,
                "consolidated_scripts": execution.consolidated_scripts,
                "status": execution.status,
                "lines_saved": execution.lines_saved,
                "backup_location": execution.backup_location,
                "execution_time": execution.execution_time,
                "validation_results": execution.validation_results
            })
        
        # Save results
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = self.reports_dir / f"consolidation_execution_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.execution_results, f, indent=2, ensure_ascii=False)
        
        return str(results_file)
    
    def run_consolidation_campaign(self, max_consolidations: int = 5) -> Dict[str, Any]:
        """Execute comprehensive consolidation campaign"""
        print("ğŸ¯ CONSOLIDATION IMPLEMENTATION CAMPAIGN")
        print("=" * 80)
        print(f"ğŸ¯ Target: Execute top {max_consolidations} consolidation opportunities")
        print("ğŸ›¡ï¸  Safety: External backups + rollback capability")
        print("ğŸ“Š Method: Archive duplicate scripts, preserve primary scripts")
        print("=" * 80)
        
        # Execute consolidations
        executions = self.execute_high_priority_consolidations(max_consolidations)
        
        # Save results
        results_file = self.save_execution_results(executions)
        
        # Display summary
        successful = len([e for e in executions if e.status == "SUCCESS"])
        total_savings = sum(e.lines_saved for e in executions if e.status == "SUCCESS")
        
        print("\nğŸ¯ CONSOLIDATION CAMPAIGN SUMMARY")
        print("=" * 80)
        print(f"ğŸ“Š Consolidations Attempted: {len(executions)}")
        print(f"âœ… Successful Consolidations: {successful}")
        print(f"ğŸ’¾ Total Lines Saved: {total_savings}")
        print(f"ğŸ“ˆ Success Rate: {successful/len(executions)*100 if executions else 0:.1f}%")
        print(f"ğŸ“‹ Results saved to: {results_file}")
        
        if executions:
            print("\nğŸ† CONSOLIDATION EXECUTION RESULTS:")
            for execution in executions[:5]:  # Show top 5
                status_icon = "âœ…" if execution.status == "SUCCESS" else "âŒ"
                print(f"{status_icon} {Path(execution.primary_script).name}")
                print(f"   ğŸ“Š Lines Saved: {execution.lines_saved}")
                print(f"   ğŸ”— Scripts Consolidated: {len(execution.consolidated_scripts)}")
                print(f"   â±ï¸  Execution Time: {execution.execution_time:.2f}s")
                print(f"   ğŸ’¾ Backup: {Path(execution.backup_location).name}")
        
        return self.execution_results

def main():
    """Main execution function"""
    workspace_path = "e:/gh_COPILOT"
    
    print("ğŸ”§ CONSOLIDATION IMPLEMENTATION ENGINE")
    print("=" * 80)
    print("Purpose: Execute script consolidation based on modulation analysis")
    print("Target: Implement Step 4 - Consolidation Project Continuation")
    print("Safety: External backups + rollback capability")
    print("=" * 80)
    
    try:
        engine = ConsolidationEngine(workspace_path)
        results = engine.run_consolidation_campaign(max_consolidations=5)
        
        if results["successful_consolidations"] > 0:
            print("\nğŸ‰ SUCCESS: Consolidation campaign completed!")
            print(f"âœ… Successfully consolidated {results['successful_consolidations']} script groups")
            print(f"ğŸ’¾ Total lines saved: {results['total_lines_saved']}")
            print("ğŸš€ Workspace is now more organized and maintainable!")
        else:
            print("\nğŸ“Š CAMPAIGN COMPLETE: No consolidations executed")
            print("âœ… All scripts are already optimally organized")
        
        return results
        
    except FileNotFoundError as e:
        print(f"\nâŒ ERROR: {str(e)}")
        print("ğŸ”§ Please run script_modulation_analyzer.py first to generate analysis results")
        return None
    except Exception as e:
        print(f"\nâŒ UNEXPECTED ERROR: {str(e)}")
        return None

if __name__ == "__main__":
    main()
