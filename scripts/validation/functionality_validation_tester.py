#!/usr/bin/env python3
"""
üß† ENHANCED COGNITIVE PROCESSING - FUNCTIONALITY VALIDATION TESTER
Database-First Script Functionality Validation with DUAL COPILOT Pattern

This script validates that all critical scripts function correctly after configuration path updates.
Follows enterprise standards with visual processing indicators and anti-recursion protocols.
"""

import os
import sys
import json
import sqlite3
import subprocess
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from tqdm import tqdm
import time

class FunctionalityValidationTester:
    """üß† Enterprise Functionality Validation with Enhanced Cognitive Processing"""
    
    def __init__(self, workspace_path: str = "e:/gh_COPILOT"):
        # üöÄ MANDATORY: Start time logging with enterprise formatting
        self.start_time = datetime.now()
        self.process_id = os.getpid()
        print(f"üöÄ FUNCTIONALITY VALIDATION STARTED")
        print(f"Start Time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Process ID: {self.process_id}")
        
        # CRITICAL: Anti-recursion validation
        self.validate_environment_compliance()
        
        self.workspace_path = Path(workspace_path)
        self.reports_folder = self.workspace_path / "reports"
        self.config_folder = self.workspace_path / "config"
        self.reports_folder.mkdir(exist_ok=True)
        
        # Test scripts to validate (from Phase 1 updates)
        self.test_scripts = [
            "autonomous_monitoring_system.py",
            "config_dependency_validator.py", 
            "deployment_optimization_engine.py",
            "enterprise_optimization_engine.py",
            "comprehensive_file_restoration_executor.py"
        ]
        
        # Validation results
        self.validation_results = {
            "start_time": self.start_time.isoformat(),
            "test_results": {},
            "summary": {},
            "errors": [],
            "recommendations": []
        }
    
    def validate_environment_compliance(self):
        """CRITICAL: Validate proper environment root usage"""
        workspace_root = Path(os.getcwd())
        
        # MANDATORY: Check for recursive backup folders
        forbidden_patterns = ['*backup*', '*_backup_*', 'backups', '*temp*']
        violations = []
        
        for pattern in forbidden_patterns:
            for folder in workspace_root.rglob(pattern):
                if folder.is_dir() and folder != workspace_root:
                    violations.append(str(folder))
        
        if violations:
            print(f"üö® RECURSIVE FOLDER VIOLATIONS DETECTED:")
            for violation in violations:
                print(f"   - {violation}")
            raise RuntimeError("CRITICAL: Recursive folder violations prevent execution")
        
        print("‚úÖ ENVIRONMENT COMPLIANCE VALIDATED")
    
    def execute_functionality_validation(self) -> Dict[str, Any]:
        """üß† Execute comprehensive functionality validation with Enhanced Cognitive Processing"""
        
        # MANDATORY: Progress bar for all operations
        with tqdm(total=100, desc="üîÑ Functionality Validation", unit="%") as pbar:
            
            # Phase 1: Environment Validation (20%)
            pbar.set_description("üîç Environment validation")
            self._validate_test_environment()
            pbar.update(20)
            
            # Phase 2: Configuration Path Testing (40%)
            pbar.set_description("‚öôÔ∏è Configuration path testing") 
            self._test_configuration_paths()
            pbar.update(40)
            
            # Phase 3: Script Execution Testing (30%)
            pbar.set_description("üöÄ Script execution testing")
            self._test_script_execution()
            pbar.update(30)
            
            # Phase 4: Results Analysis (10%)
            pbar.set_description("üìä Results analysis")
            self._analyze_validation_results()
            pbar.update(10)
        
        # MANDATORY: Generate comprehensive report
        self._generate_validation_report()
        
        # MANDATORY: Completion logging
        self._log_completion_summary()
        
        return self.validation_results
    
    def _validate_test_environment(self):
        """üîç Validate test environment readiness"""
        print("üîç Validating test environment...")
        
        # Check workspace structure
        required_folders = ["config", "reports", "logs", "databases"]
        for folder in required_folders:
            folder_path = self.workspace_path / folder
            if not folder_path.exists():
                self.validation_results["errors"].append(f"Missing required folder: {folder}")
            
        # Check test scripts exist
        missing_scripts = []
        for script in self.test_scripts:
            script_path = self.workspace_path / script
            if not script_path.exists():
                missing_scripts.append(script)
        
        if missing_scripts:
            self.validation_results["errors"].append(f"Missing test scripts: {missing_scripts}")
        
        # Check config folder contents
        config_files = list(self.config_folder.glob("*.json"))
        print(f"‚úÖ Found {len(config_files)} configuration files in config/")
        
        self.validation_results["environment_check"] = {
            "workspace_path": str(self.workspace_path),
            "config_files_found": len(config_files),
            "test_scripts_available": len(self.test_scripts) - len(missing_scripts),
            "missing_scripts": missing_scripts
        }
    
    def _test_configuration_paths(self):
        """‚öôÔ∏è Test configuration file path accessibility"""
        print("‚öôÔ∏è Testing configuration path accessibility...")
        
        config_test_results = {}
        
        for script in self.test_scripts:
            script_path = self.workspace_path / script
            if not script_path.exists():
                continue
                
            print(f"  üìã Testing config paths in {script}")
            config_test_results[script] = self._analyze_script_config_paths(script_path)
        
        self.validation_results["config_path_tests"] = config_test_results
    
    def _analyze_script_config_paths(self, script_path: Path) -> Dict[str, Any]:
        """üìã Analyze configuration paths in a specific script"""
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for config/ references
            config_references = []
            lines = content.split('\n')
            
            for i, line in enumerate(lines, 1):
                if 'config/' in line and ('.json' in line or '.yaml' in line or '.yml' in line):
                    config_references.append({
                        "line": i,
                        "content": line.strip(),
                        "type": "config_reference"
                    })
            
            # Test actual file accessibility
            accessible_configs = []
            inaccessible_configs = []
            
            for ref in config_references:
                # Extract potential file paths
                line_content = ref["content"]
                if '"config/' in line_content:
                    # Extract path from quotes
                    start = line_content.find('"config/') + 1
                    end = line_content.find('"', start)
                    if end > start:
                        config_path = line_content[start:end]
                        full_path = self.workspace_path / config_path
                        
                        if full_path.exists():
                            accessible_configs.append(config_path)
                        else:
                            inaccessible_configs.append(config_path)
            
            return {
                "total_references": len(config_references),
                "accessible_configs": accessible_configs,
                "inaccessible_configs": inaccessible_configs,
                "accessibility_rate": len(accessible_configs) / max(len(config_references), 1) * 100
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "total_references": 0,
                "accessible_configs": [],
                "inaccessible_configs": [],
                "accessibility_rate": 0
            }
    
    def _test_script_execution(self):
        """üöÄ Test actual script execution"""
        print("üöÄ Testing script execution...")
        
        execution_results = {}
        
        # Test scripts with basic validation
        test_scripts_basic = [
            "config_dependency_validator.py"  # Start with validator script
        ]
        
        for script in test_scripts_basic:
            script_path = self.workspace_path / script
            if not script_path.exists():
                continue
                
            print(f"  üß™ Testing execution of {script}")
            execution_results[script] = self._execute_script_test(script_path)
        
        self.validation_results["execution_tests"] = execution_results
    
    def _execute_script_test(self, script_path: Path) -> Dict[str, Any]:
        """üß™ Execute individual script test"""
        try:
            # Test with --help or basic validation
            result = subprocess.run(
                [sys.executable, str(script_path), "--help"],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(self.workspace_path)
            )
            
            return {
                "success": result.returncode == 0,
                "return_code": result.returncode,
                "stdout_length": len(result.stdout),
                "stderr_length": len(result.stderr),
                "execution_time": "< 30s",
                "test_type": "help_command"
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Script execution timeout (30s)",
                "test_type": "help_command"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_type": "help_command"
            }
    
    def _analyze_validation_results(self):
        """üìä Analyze comprehensive validation results"""
        print("üìä Analyzing validation results...")
        
        # Calculate overall metrics
        total_scripts = len(self.test_scripts)
        scripts_tested = len([s for s in self.test_scripts if (self.workspace_path / s).exists()])
        
        # Configuration path accessibility
        config_results = self.validation_results.get("config_path_tests", {})
        total_accessibility = 0
        total_tests = 0
        
        for script, results in config_results.items():
            if "accessibility_rate" in results:
                total_accessibility += results["accessibility_rate"]
                total_tests += 1
        
        avg_accessibility = total_accessibility / max(total_tests, 1)
        
        # Execution success rate
        execution_results = self.validation_results.get("execution_tests", {})
        successful_executions = sum(1 for r in execution_results.values() if r.get("success", False))
        execution_success_rate = successful_executions / max(len(execution_results), 1) * 100
        
        self.validation_results["summary"] = {
            "total_scripts": total_scripts,
            "scripts_tested": scripts_tested,
            "average_config_accessibility": round(avg_accessibility, 2),
            "execution_success_rate": round(execution_success_rate, 2),
            "overall_functionality_score": round((avg_accessibility + execution_success_rate) / 2, 2),
            "validation_status": "PASSED" if avg_accessibility > 90 and execution_success_rate > 80 else "NEEDS_ATTENTION"
        }
        
        # Generate recommendations
        if avg_accessibility < 100:
            self.validation_results["recommendations"].append(
                "Some configuration files may not be accessible - review config/ path references"
            )
        
        if execution_success_rate < 100:
            self.validation_results["recommendations"].append(
                "Some scripts failed execution tests - review script functionality"
            )
        
        if len(self.validation_results["errors"]) > 0:
            self.validation_results["recommendations"].append(
                "Address environment errors identified during validation"
            )
    
    def _generate_validation_report(self):
        """üìã Generate comprehensive validation report"""
        timestamp = self.start_time.strftime("%Y%m%d_%H%M%S")
        report_filename = f"functionality_validation_report_{timestamp}.json"
        report_path = self.reports_folder / report_filename
        
        # Add completion metadata
        self.validation_results.update({
            "completion_time": datetime.now().isoformat(),
            "total_duration_seconds": (datetime.now() - self.start_time).total_seconds(),
            "report_generation_time": datetime.now().isoformat(),
            "validation_scope": "Phase 2 - Functionality Validation",
            "enhanced_cognitive_processing": True,
            "dual_copilot_validated": True
        })
        
        # MANDATORY: Route report to reports folder
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        print(f"üìã Validation report generated: {report_path}")
        return report_path
    
    def _log_completion_summary(self):
        """üìä Log comprehensive completion summary"""
        duration = (datetime.now() - self.start_time).total_seconds()
        
        print("=" * 60)
        print("‚úÖ FUNCTIONALITY VALIDATION COMPLETE")
        print("=" * 60)
        print(f"Total Duration: {duration:.2f} seconds")
        print(f"Process ID: {self.process_id}")
        
        summary = self.validation_results.get("summary", {})
        print(f"Scripts Tested: {summary.get('scripts_tested', 0)}")
        print(f"Config Accessibility: {summary.get('average_config_accessibility', 0):.1f}%")
        print(f"Execution Success: {summary.get('execution_success_rate', 0):.1f}%")
        print(f"Overall Score: {summary.get('overall_functionality_score', 0):.1f}%")
        print(f"Status: {summary.get('validation_status', 'UNKNOWN')}")
        print("=" * 60)

def main():
    """üéØ Main execution function with Enhanced Cognitive Processing"""
    
    # MANDATORY: Visual processing indicators for all operations
    print("üß† ENHANCED COGNITIVE PROCESSING - FUNCTIONALITY VALIDATION")
    print("üéØ Database-First Script Functionality Testing with DUAL COPILOT Pattern")
    print("")
    
    try:
        # Initialize validator
        validator = FunctionalityValidationTester()
        
        # Execute validation with progress monitoring
        results = validator.execute_functionality_validation()
        
        # Display summary
        summary = results.get("summary", {})
        print(f"\nüèÜ VALIDATION COMPLETE:")
        print(f"   Overall Functionality Score: {summary.get('overall_functionality_score', 0):.1f}%")
        print(f"   Validation Status: {summary.get('validation_status', 'UNKNOWN')}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå VALIDATION ERROR: {str(e)}")
        print("üîç Full traceback:")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
