#!/usr/bin/env python3
"""
üîß REFINED ENTERPRISE CONTINUATION PROCESSOR
Large-Scale Violation Processing with Enhanced Success Patterns

Author: Enterprise Violation Processing System  
Date: July 13, 2025
Status: PRODUCTION READY - REFINED DEPLOYMENT

ENHANCEMENTS:
- Anti-recursion safety protocols
- External backup root enforcement
- Enhanced pattern matching for critical violations
- Intelligent batch processing for optimal throughput
"""

import os
import re
import sys
import sqlite3
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple
from tqdm import tqdm

# Configure enterprise logging with UTF-8 encoding
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('refined_enterprise_processing.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class RefinedEnterpriseProcessor:
    """üîß Refined Enterprise Processor - Large-Scale Violation Processing"""

    def __init__(self, workspace_path: str = "e:/gh_COPILOT"):
        self.workspace_path = Path(workspace_path)
        self.database_path = self.workspace_path / "databases" / "flake8_violations.db"
        
        # CRITICAL: External backup root only
        self.backup_root = Path("e:/temp/gh_COPILOT_Refined_Backups")
        self.backup_root.mkdir(parents=True, exist_ok=True)
        
        # Validate environment
        self.validate_environment()
        
        # High-impact violation types for refined processing
        self.priority_violations = [
            'E999',  # SyntaxError
            'E902',  # IOError
            'F821',  # undefined name
            'F822',  # duplicate argument
            'F823',  # local variable referenced before assignment
        ]
        
        # Standard violations for batch processing
        self.standard_violations = [
            'W291',  # trailing whitespace
            'W293',  # blank line contains whitespace
            'W292',  # no newline at end of file
            'E501',  # line too long
            'E302',  # expected 2 blank lines
            'E303',  # too many blank lines
        ]
        
        self.session_id = f"refined_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info("üîß REFINED ENTERPRISE PROCESSOR INITIALIZED")
        logger.info(f"Session ID: {self.session_id}")
        logger.info(f"External Backup Root: {self.backup_root}")
        logger.info("Processing Mode: Large-Scale Continuation")

    def validate_environment(self):
        """üõ°Ô∏è Validate environment with anti-recursion checks"""
        # Check for recursive backup folders
        workspace_backups = list(self.workspace_path.rglob("*backup*"))
        if workspace_backups:
            logger.error(f"üö® CRITICAL: Found {len(workspace_backups)} backup folders in workspace!")
            for backup in workspace_backups:
                logger.error(f"   - {backup}")
            raise RuntimeError("CRITICAL: Recursive backup violations detected")
        
        # Validate database exists
        if not self.database_path.exists():
            logger.warning("‚ö†Ô∏è Database not found, will create if needed")
        
        # Validate external backup root
        if not str(self.backup_root).startswith("e:/temp/"):
            logger.error(f"üö® CRITICAL: Invalid backup root: {self.backup_root}")
            raise RuntimeError("CRITICAL: Backup root must be external")
        
        logger.info("‚úÖ Environment validation passed")

    def get_continuation_batches(self, max_batches: int = 50) -> List[Dict]:
        """üìä Get batches for continuation processing"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                # Get priority violations first
                priority_placeholders = ','.join(['?' for _ in self.priority_violations])
                cursor.execute(f"""
                    SELECT file_path, line_number, error_code, line_content
                    FROM violations 
                    WHERE error_code IN ({priority_placeholders})
                    AND status = 'PENDING'
                    ORDER BY error_code, file_path, line_number
                    LIMIT ?
                """, tuple(self.priority_violations) + (max_batches * 10,))
                
                priority_violations = cursor.fetchall()
                
                # Get standard violations for remaining capacity
                remaining_capacity = max(0, (max_batches * 20) - len(priority_violations))
                standard_placeholders = ','.join(['?' for _ in self.standard_violations])
                
                if remaining_capacity > 0:
                    cursor.execute(f"""
                        SELECT file_path, line_number, error_code, line_content
                        FROM violations 
                        WHERE error_code IN ({standard_placeholders})
                        AND status = 'PENDING'
                        ORDER BY file_path, line_number
                        LIMIT ?
                    """, tuple(self.standard_violations) + (remaining_capacity,))
                    
                    standard_violations = cursor.fetchall()
                else:
                    standard_violations = []
                
                # Combine and group into batches
                all_violations = priority_violations + standard_violations
                batches = self._group_violations_into_batches(all_violations, max_batches)
                
                logger.info(f"üìä Created {len(batches)} continuation batches")
                logger.info(f"üéØ Priority violations: {len(priority_violations)}")
                logger.info(f"üìã Standard violations: {len(standard_violations)}")
                
                return batches
                
        except Exception as e:
            logger.error(f"‚ùå Failed to get continuation batches: {e}")
            return []

    def _group_violations_into_batches(self, violations: List[Tuple], max_batches: int) -> List[Dict]:
        """üì¶ Group violations into processing batches"""
        batches = []
        current_batch = {}
        current_file = None
        
        for file_path, line_number, error_code, line_content in violations:
            if file_path != current_file:
                if current_batch:
                    batches.append(current_batch)
                    if len(batches) >= max_batches:
                        break
                
                current_batch = {
                    'file_path': file_path,
                    'violations': [],
                    'priority_count': 0,
                    'standard_count': 0
                }
                current_file = file_path
            
            violation_data = {
                'line_number': line_number,
                'error_code': error_code,
                'line_content': line_content
            }
            
            current_batch['violations'].append(violation_data)
            
            if error_code in self.priority_violations:
                current_batch['priority_count'] += 1
            else:
                current_batch['standard_count'] += 1
        
        if current_batch and len(batches) < max_batches:
            batches.append(current_batch)
        
        return batches

    def apply_refined_fixes(self, batch: Dict) -> Tuple[int, int, List[str]]:
        """üîß Apply refined fixes with enhanced success patterns"""
        file_path = batch['file_path']
        violations = batch['violations']
        
        try:
            # Create backup
            backup_path = self.create_refined_backup(file_path)
            
            # Read file
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            fixed_count = 0
            fixed_violations = []
            
            # Sort violations by line number (reverse order to maintain line numbers)
            sorted_violations = sorted(violations, key=lambda x: x['line_number'], reverse=True)
            
            # Apply fixes
            for violation in sorted_violations:
                line_idx = violation['line_number'] - 1
                
                if 0 <= line_idx < len(lines):
                    original_line = lines[line_idx]
                    fixed_line = self.apply_refined_fix(
                        original_line, 
                        violation['error_code'],
                        violation.get('line_content', '')
                    )
                    
                    if fixed_line != original_line:
                        lines[line_idx] = fixed_line
                        fixed_count += 1
                        fixed_violations.append({
                            'file_path': file_path,
                            'line_number': violation['line_number'],
                            'error_code': violation['error_code'],
                            'backup_path': backup_path
                        })
            
            # Write fixed file
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            
            # Update database
            self.update_violation_status_refined(fixed_violations, 'FIXED')
            
            return fixed_count, len(violations), [str(backup_path)]
            
        except Exception as e:
            logger.error(f"‚ùå Failed to apply refined fixes to {file_path}: {e}")
            return 0, len(violations), []

    def apply_refined_fix(self, line: str, error_code: str, line_content: str = "") -> str:
        """üéØ Apply refined fixes for enhanced success"""
        if error_code == 'W291':  # trailing whitespace
            return line.rstrip() + '\n' if line.endswith('\n') else line.rstrip()
        
        elif error_code == 'W293':  # blank line contains whitespace
            if line.strip() == '':
                return '\n' if line.endswith('\n') else ''
        
        elif error_code == 'W292':  # no newline at end of file
            if not line.endswith('\n'):
                return line + '\n'
        
        elif error_code == 'E501':  # line too long
            if len(line) > 79:
                # Simple line breaking for common patterns
                if ',' in line and len(line) < 120:
                    # Find good break point after comma
                    break_point = line.rfind(',', 0, 75)
                    if break_point > 50:
                        return line[:break_point + 1] + '\n' + '    ' + line[break_point + 1:].lstrip()
        
        elif error_code == 'E302':  # expected 2 blank lines
            return '\n\n' + line
        
        elif error_code == 'E303':  # too many blank lines
            if line.strip() == '':
                return ''
        
        return line

    def update_violation_status_refined(self, fixes_applied: List[Dict], status: str):
        """üìä Update violation status in database"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                for fix in fixes_applied:
                    cursor.execute("""
                        UPDATE violations 
                        SET status = ?, fixed_date = ?
                        WHERE file_path = ? AND line_number = ? AND error_code = ?
                    """, (status, datetime.now().isoformat(), 
                          fix['file_path'], fix['line_number'], fix['error_code']))
                
                conn.commit()
                logger.info(f"üìä Updated {len(fixes_applied)} violation statuses to {status}")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to update violation status: {e}")

    def create_refined_backup(self, file_path: str) -> str:
        """üíæ Create external backup with refined naming"""
        file_path_obj = Path(file_path)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"{file_path_obj.stem}_refined_{timestamp}{file_path_obj.suffix}"
        backup_path = self.backup_root / backup_filename
        
        try:
            shutil.copy2(file_path, backup_path)
            logger.info(f"üíæ Created backup: {backup_path}")
            return str(backup_path)
        except Exception as e:
            logger.error(f"‚ùå Backup failed: {e}")
            return ""

    def execute_refined_processing(self, max_batches: int = 50) -> Dict[str, Any]:
        """üîß Execute refined continuation processing"""
        start_time = datetime.now()
        logger.info("üîß STARTING REFINED CONTINUATION PROCESSING")
        logger.info(f"Target: {max_batches} batches maximum")
        logger.info(f"Focus: Priority + Standard violations")
        
        results = self._create_empty_results()
        
        try:
            # Get continuation batches
            logger.info("üìä Getting continuation violation batches...")
            batches = self.get_continuation_batches(max_batches)
            
            if not batches:
                logger.warning("‚ö†Ô∏è No continuation batches found")
                return results
            
            # Process batches with progress tracking
            total_fixed = 0
            total_attempted = 0
            all_backups = []
            priority_fixed = 0
            standard_fixed = 0
            
            with tqdm(total=len(batches), desc="üîß Refined Processing", unit="batch") as pbar:
                for i, batch in enumerate(batches):
                    pbar.set_description(f"üîß Processing {Path(batch['file_path']).name}")
                    
                    fixed_count, attempted_count, backups = self.apply_refined_fixes(batch)
                    
                    total_fixed += fixed_count
                    total_attempted += attempted_count
                    all_backups.extend(backups)
                    
                    # Track priority vs standard fixes
                    if batch.get('priority_count', 0) > 0:
                        priority_fixed += min(fixed_count, batch['priority_count'])
                    if batch.get('standard_count', 0) > 0:
                        standard_fixed += min(fixed_count - min(fixed_count, batch.get('priority_count', 0)), 
                                            batch['standard_count'])
                    
                    # Update progress
                    pbar.update(1)
                    success_rate = (total_fixed / total_attempted * 100) if total_attempted > 0 else 0
                    pbar.set_postfix({"Success": f"{success_rate:.1f}%", "Fixed": total_fixed})
            
            # Calculate final results
            duration = (datetime.now() - start_time).total_seconds()
            success_rate = (total_fixed / total_attempted * 100) if total_attempted > 0 else 0
            
            results.update({
                'session_id': self.session_id,
                'batches_processed': len(batches),
                'violations_fixed': total_fixed,
                'violations_attempted': total_attempted,
                'priority_violations_fixed': priority_fixed,
                'standard_violations_fixed': standard_fixed,
                'success_rate': success_rate,
                'processing_duration': duration,
                'backups_created': len(all_backups),
                'backup_paths': all_backups,
                'processing_type': 'REFINED_CONTINUATION',
                'status': 'COMPLETED_SUCCESS' if success_rate >= 80 else 'COMPLETED_PARTIAL'
            })
            
            # Log final summary
            logger.info("‚úÖ REFINED CONTINUATION PROCESSING COMPLETED")
            logger.info(f"üìä Success Rate: {success_rate:.1f}%")
            logger.info(f"üîß Fixed: {total_fixed}/{total_attempted} violations")
            logger.info(f"üéØ Priority: {priority_fixed}, Standard: {standard_fixed}")
            logger.info(f"‚è±Ô∏è Duration: {duration:.1f} seconds")
            logger.info(f"üíæ Backups: {len(all_backups)} created")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Refined processing failed: {e}")
            results['status'] = 'FAILED'
            results['error'] = str(e)
            return results

    def _create_empty_results(self) -> Dict[str, Any]:
        """üìä Create empty results structure"""
        return {
            'session_id': self.session_id,
            'batches_processed': 0,
            'violations_fixed': 0,
            'violations_attempted': 0,
            'priority_violations_fixed': 0,
            'standard_violations_fixed': 0,
            'success_rate': 0.0,
            'processing_duration': 0.0,
            'backups_created': 0,
            'backup_paths': [],
            'processing_type': 'REFINED_CONTINUATION',
            'status': 'INITIALIZED'
        }


def main():
    """üîß Main refined processing execution"""
    try:
        processor = RefinedEnterpriseProcessor()
        results = processor.execute_refined_processing(max_batches=50)
        
        print("=" * 80)
        print("üîß REFINED CONTINUATION PROCESSING COMPLETE")
        print("=" * 80)
        print(f"Session ID: {results['session_id']}")
        print(f"Status: {results['status']}")
        print(f"Success Rate: {results['success_rate']:.1f}%")
        print(f"Violations Fixed: {results['violations_fixed']}")
        print(f"Priority Fixed: {results['priority_violations_fixed']}")
        print(f"Standard Fixed: {results['standard_violations_fixed']}")
        print(f"Batches Processed: {results['batches_processed']}")
        print(f"Duration: {results['processing_duration']:.1f} seconds")
        print(f"Backups Created: {results['backups_created']}")
        print("=" * 80)
        
        return results['status'] == 'COMPLETED_SUCCESS'
        
    except Exception as e:
        logger.error(f"‚ùå Main execution failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
