#!/usr/bin/env python3
"""
üßπ LEGACY DEPLOYMENT ORCHESTRATOR CLEANUP SCRIPT
===============================================
Cleanup and archival of legacy deployment orchestrator scripts.

This script identifies and archives all legacy deployment orchestrator scripts
now that they have been consolidated into UNIFIED_DEPLOYMENT_ORCHESTRATOR_CONSOLIDATED.py

üéØ DUAL COPILOT PATTERN: Primary Archival + Secondary Validation
üé¨ Visual Processing Indicators: MANDATORY
üõ°Ô∏è Anti-Recursion Protection: ENABLED
üì¶ Backup Creation: ENABLED

Version: 1.0.0
Created: January 2, 2025
"""

import os
import sys
import json
import shutil
import logging
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
import zipfile

# Configure logging
logging.basicConfig(]
    format = '%(asctime)s - %(levelname)s - %(message)s',
    handlers = [
        logging.FileHandler('legacy_cleanup.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class LegacyCleanupConfig:
    """üîß Configuration for legacy deployment orchestrator cleanup"""

    # Paths
    workspace_root: str = "e:\\gh_COPILOT"
    archive_root: str = "e:\\gh_COPILOT\\backup\\legacy_deployment_orchestrators"

    # Cleanup session
    cleanup_session_id: str = field(]
        default_factory = lambda: f"CLEANUP_{int(datetime.now().timestamp())}")

    # Backup options
    create_backup_zip: bool = True
    verify_checksums: bool = True

    # Safety options
    dry_run: bool = False
    require_confirmation: bool = True


@dataclass
class LegacyScript:
    """üìÑ Information about a legacy deployment orchestrator script"""

    file_path: Path
    file_name: str
    file_size: int
    checksum: str
    last_modified: datetime
    script_type: str
    consolidation_status: str


class LegacyDeploymentOrchestratorCleanup:
    """üßπ Legacy deployment orchestrator cleanup manager"""

    def __init__(self, config: Optional[LegacyCleanupConfig]=None):
        """üîß Initialize cleanup manager"""

        self.config = config or LegacyCleanupConfig()
        self.cleanup_session_id = self.config.cleanup_session_id
        self.start_time = datetime.now()

        # Legacy deployment orchestrator patterns
        self.legacy_patterns = [
        ]

        # Files to EXCLUDE from cleanup (keep these)
        self.exclude_patterns = [
        ]

        # Initialize tracking
        self.legacy_scripts: List[LegacyScript] = [
        self.cleanup_results = {
            "start_time": self.start_time.isoformat(),
            "scripts_identified": 0,
            "scripts_archived": 0,
            "scripts_removed": 0,
            "backup_created": False,
            "archive_path": None,
            "errors": [],
            "warnings": []
        }

        logger.info("üßπ LEGACY DEPLOYMENT ORCHESTRATOR CLEANUP INITIATED")
        logger.info(f"Cleanup Session ID: {self.cleanup_session_id}")
        logger.info(f"Workspace Root: {self.config.workspace_root}")
        logger.info(f"Archive Root: {self.config.archive_root}")
        logger.info(f"Dry Run: {self.config.dry_run}")

    def identify_legacy_scripts(self) -> List[LegacyScript]:
        """üîç Identify all legacy deployment orchestrator scripts"""

        logger.info("üîç IDENTIFYING LEGACY DEPLOYMENT ORCHESTRATOR SCRIPTS...")

        workspace_path = Path(self.config.workspace_root)
        identified_scripts = [

        # Search for legacy scripts using patterns
        for pattern in self.legacy_patterns:
            for script_path in workspace_path.glob(pattern):
                if script_path.is_file():
                    # Check if it should be excluded
                    should_exclude = False
                    for exclude_pattern in self.exclude_patterns:
                        if script_path.match(exclude_pattern.replace("**/", "")):
                            should_exclude = True
                            break

                    if not should_exclude:
                        # Create legacy script record
                        legacy_script = self._create_legacy_script_record(]
                            script_path)
                        identified_scripts.append(legacy_script)
                        logger.info(
                            f"üìÑ Found legacy script: {script_path.relative_to(workspace_path)}")

        self.legacy_scripts = identified_scripts
        self.cleanup_results["scripts_identified"] = len(identified_scripts)

        logger.info(
            f"‚úÖ Identified {len(identified_scripts)} legacy deployment orchestrator scripts")
        return identified_scripts

    def _create_legacy_script_record(self, script_path: Path) -> LegacyScript:
        """üìã Create a record for a legacy script"""

        file_stats = script_path.stat()

        # Calculate checksum
        checksum = self._calculate_file_checksum(script_path)

        # Determine script type
        script_type = self._determine_script_type(script_path)

        # Check consolidation status
        consolidation_status = self._check_consolidation_status(script_path)

        return LegacyScript(]
            last_modified = datetime.fromtimestamp(file_stats.st_mtime),
            script_type = script_type,
            consolidation_status = consolidation_status
        )

    def _calculate_file_checksum(self, file_path: Path) -> str:
        """üîê Calculate SHA256 checksum for a file"""

        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è Could not calculate checksum for {file_path}: {e}")
            return "ERROR"

    def _determine_script_type(self, script_path: Path) -> str:
        """üè∑Ô∏è Determine the type of deployment orchestrator script"""

        file_name = script_path.name.lower()

        if "enterprise_gh_copilot_deployment_orchestrator" in file_name:
            return "Enterprise GH Copilot Deployment Orchestrator"
        elif "integrated_deployment_orchestrator" in file_name:
            return "Integrated Deployment Orchestrator"
        elif "production_deployment_orchestrator" in file_name:
            return "Production Deployment Orchestrator"
        elif "comprehensive_deployment_manager" in file_name:
            return "Comprehensive Deployment Manager"
        elif "final_staging_deployment_orchestrator" in file_name:
            return "Final Staging Deployment Orchestrator"
        elif "final_enterprise_deployment_executor" in file_name:
            return "Final Enterprise Deployment Executor"
        elif "enterprise_intelligence_deployment_orchestrator" in file_name:
            return "Enterprise Intelligence Deployment Orchestrator"
        else:
            return "Generic Deployment Orchestrator"

    def _check_consolidation_status(self, script_path: Path) -> str:
        """‚úÖ Check if script functionality is consolidated"""

        try:
            # Read the script content to check for deprecation notices
            with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            if "DEPRECATED" in content or "MIGRATED" in content:
                return "DEPRECATED"
            elif "unified_deployment_orchestrator" in content:
                return "REFERENCES_UNIFIED"
            else:
                return "NEEDS_CONSOLIDATION"

        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è Could not check consolidation status for {script_path}: {e}")
            return "UNKNOWN"

    def create_archive_backup(self) -> bool:
        """üì¶ Create a backup archive of all legacy scripts"""

        if not self.config.create_backup_zip:
            logger.info("‚è© Backup creation disabled")
            return True

        logger.info("üì¶ CREATING BACKUP ARCHIVE...")

        try:
            # Create archive directory
            archive_path = Path(self.config.archive_root)
            archive_path.mkdir(parents=True, exist_ok=True)

            # Create zip archive
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_filename = f"legacy_deployment_orchestrators_backup_{timestamp}.zip"
            zip_path = archive_path / zip_filename

            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for legacy_script in self.legacy_scripts:
                    # Calculate relative path for archive
                    workspace_root = Path(self.config.workspace_root)
                    relative_path = legacy_script.file_path.relative_to(]
                        workspace_root)

                    # Add file to archive
                    zipf.write(]
                               arcname = str(relative_path))
                    logger.info(f"üìÑ Added to archive: {relative_path}")

            # Verify archive
            if zip_path.exists():
                self.cleanup_results["backup_created"] = True
                self.cleanup_results["archive_path"] = str(zip_path)
                logger.info(f"‚úÖ Backup archive created: {zip_path}")
                return True
            else:
                logger.error("‚ùå Archive creation failed")
                return False

        except Exception as e:
            logger.error(f"‚ùå Archive creation failed: {e}")
            self.cleanup_results["errors"].append(]
                f"Archive creation failed: {e}")
            return False

    def archive_legacy_scripts(self) -> bool:
        """üìÅ Archive legacy scripts to backup directory"""

        logger.info("üìÅ ARCHIVING LEGACY SCRIPTS...")

        try:
            # Create archive directory structure
            archive_path = Path(self.config.archive_root)
            session_archive = archive_path / self.cleanup_session_id
            session_archive.mkdir(parents=True, exist_ok=True)

            archived_count = 0

            for legacy_script in self.legacy_scripts:
                # Calculate relative path
                workspace_root = Path(self.config.workspace_root)
                relative_path = legacy_script.file_path.relative_to(]
                    workspace_root)

                # Create archive destination
                archive_destination = session_archive / relative_path
                archive_destination.parent.mkdir(parents=True, exist_ok=True)

                if not self.config.dry_run:
                    # Copy file to archive
                    shutil.copy2(legacy_script.file_path, archive_destination)

                    # Verify copy
                    if self.config.verify_checksums:
                        archive_checksum = self._calculate_file_checksum(]
                            archive_destination)
                        if archive_checksum != legacy_script.checksum:
                            logger.error(
                                f"‚ùå Checksum mismatch for {legacy_script.file_name}")
                            continue

                logger.info(f"üìÑ Archived: {relative_path}")
                archived_count += 1

            self.cleanup_results["scripts_archived"] = archived_count
            logger.info(f"‚úÖ Archived {archived_count} legacy scripts")
            return True

        except Exception as e:
            logger.error(f"‚ùå Archive process failed: {e}")
            self.cleanup_results["errors"].append(]
                f"Archive process failed: {e}")
            return False

    def remove_legacy_scripts(self) -> bool:
        """üóëÔ∏è Remove legacy scripts from workspace"""

        if self.config.require_confirmation:
            logger.info("‚ö†Ô∏è CONFIRMATION REQUIRED")
            logger.info(
                f"About to remove {len(self.legacy_scripts)} legacy deployment orchestrator scripts")

            if not self.config.dry_run:
                response = input(]
                    "Proceed with removal? (yes/no): ").strip().lower()
                if response != 'yes':
                    logger.info("‚ùå Removal cancelled by user")
                    return False

        logger.info("üóëÔ∏è REMOVING LEGACY SCRIPTS...")

        try:
            removed_count = 0

            for legacy_script in self.legacy_scripts:
                if not self.config.dry_run:
                    # Remove the file
                    legacy_script.file_path.unlink()

                    # Verify removal
                    if legacy_script.file_path.exists():
                        logger.error(
                            f"‚ùå Failed to remove {legacy_script.file_name}")
                        continue

                logger.info(f"üóëÔ∏è Removed: {legacy_script.file_path.name}")
                removed_count += 1

            self.cleanup_results["scripts_removed"] = removed_count
            logger.info(f"‚úÖ Removed {removed_count} legacy scripts")
            return True

        except Exception as e:
            logger.error(f"‚ùå Removal process failed: {e}")
            self.cleanup_results["errors"].append(]
                f"Removal process failed: {e}")
            return False

    def generate_cleanup_report(self) -> Dict[str, Any]:
        """üìä Generate comprehensive cleanup report"""

        logger.info("üìä GENERATING CLEANUP REPORT...")

        # Calculate statistics
        total_size = sum(script.file_size for script in self.legacy_scripts)
        script_types = {}
        consolidation_status = {}

        for script in self.legacy_scripts:
            script_types[script.script_type] = script_types.get(]
                script.script_type, 0) + 1
            consolidation_status[script.consolidation_status] = consolidation_status.get(]
                script.consolidation_status, 0) + 1

        # Create report
        report = {
                "start_time": self.start_time.isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration": (datetime.now() - self.start_time).total_seconds()
            },
            "configuration": {},
            "statistics": {]
                "total_scripts_identified": len(self.legacy_scripts),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "scripts_by_type": script_types,
                "consolidation_status": consolidation_status
            },
            "results": self.cleanup_results,
            "legacy_scripts": []
                    "file_path": str(script.file_path),
                    "file_size": script.file_size,
                    "checksum": script.checksum,
                    "last_modified": script.last_modified.isoformat(),
                    "script_type": script.script_type,
                    "consolidation_status": script.consolidation_status
                }
                for script in self.legacy_scripts
            ],
            "recommendations": self._generate_recommendations()
        }

        # Save report
        report_path = Path(self.config.archive_root) / \
            f"cleanup_report_{self.cleanup_session_id}.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str)

        logger.info(f"üìä Cleanup report saved: {report_path}")

        return report

    def _generate_recommendations(self) -> List[str]:
        """üí° Generate recommendations based on cleanup results"""

        recommendations = [

        if self.cleanup_results["scripts_identified"] > 0:
            recommendations.append(]
                "Legacy deployment orchestrator scripts have been identified and consolidated.")

        if self.cleanup_results["backup_created"]:
            recommendations.append(]
                "Backup archive created successfully. Archive can be used for recovery if needed.")

        if any(script.consolidation_status == "NEEDS_CONSOLIDATION" for script in self.legacy_scripts):
            recommendations.append(]
                "Some scripts may need manual review to ensure all functionality is consolidated.")

        recommendations.append(]
            "Update all documentation to reference only UNIFIED_DEPLOYMENT_ORCHESTRATOR_CONSOLIDATED.py")
        recommendations.append(]
            "Test the consolidated orchestrator thoroughly before production use.")
        recommendations.append(]
            "Consider removing archived scripts after successful production deployment.")

        return recommendations

    def execute_cleanup(self) -> Dict[str, Any]:
        """üöÄ Execute complete cleanup process"""

        logger.info("üöÄ EXECUTING LEGACY DEPLOYMENT ORCHESTRATOR CLEANUP...")
        logger.info("=" * 80)

        try:
            # Phase 1: Identify legacy scripts
            logger.info("üìã Phase 1: Identifying legacy scripts...")
            self.identify_legacy_scripts()

            if not self.legacy_scripts:
                logger.info(
                    "‚úÖ No legacy deployment orchestrator scripts found")
                return self.generate_cleanup_report()

            # Phase 2: Create backup archive
            logger.info("üì¶ Phase 2: Creating backup archive...")
            if not self.create_archive_backup():
                logger.error("‚ùå Backup creation failed. Aborting cleanup.")
                return self.generate_cleanup_report()

            # Phase 3: Archive to structured directory
            logger.info("üìÅ Phase 3: Archiving legacy scripts...")
            if not self.archive_legacy_scripts():
                logger.error("‚ùå Archive process failed. Aborting cleanup.")
                return self.generate_cleanup_report()

            # Phase 4: Remove legacy scripts (optional)
            logger.info("üóëÔ∏è Phase 4: Removing legacy scripts...")
            if not self.remove_legacy_scripts():
                logger.warning(
                    "‚ö†Ô∏è Some legacy scripts may not have been removed")

            # Phase 5: Generate final report
            logger.info("üìä Phase 5: Generating final report...")
            report = self.generate_cleanup_report()

            logger.info("üéâ LEGACY DEPLOYMENT ORCHESTRATOR CLEANUP COMPLETED!")
            logger.info("=" * 80)

            return report

        except Exception as e:
            logger.error(f"‚ùå Cleanup execution failed: {e}")
            self.cleanup_results["errors"].append(]
                f"Cleanup execution failed: {e}")
            return self.generate_cleanup_report()


def main():
    """üöÄ Main execution function"""

    logger.info("üßπ LEGACY DEPLOYMENT ORCHESTRATOR CLEANUP SCRIPT")
    logger.info("=" * 80)
    logger.info("DUAL COPILOT PATTERN: Primary Cleanup + Secondary Validation")
    logger.info("Visual Processing Indicators: MANDATORY")
    logger.info("Anti-Recursion Protection: ENABLED")
    logger.info("=" * 80)

    try:
        # Create configuration
        config = LegacyCleanupConfig(]
        )

        # Execute cleanup
        cleanup_manager = LegacyDeploymentOrchestratorCleanup(config)
        result = cleanup_manager.execute_cleanup()

        # Display results
        logger.info("=" * 80)
        logger.info("üéâ CLEANUP COMPLETED!")
        logger.info(
            f"Scripts Identified: {result['statistics']['total_scripts_identified']}")
        logger.info(
            f"Scripts Archived: {result['results']['scripts_archived']}")
        logger.info(f"Scripts Removed: {result['results']['scripts_removed']}")
        logger.info(f"Backup Created: {result['results']['backup_created']}")
        logger.info(
            f"Total Size Cleaned: {result['statistics']['total_size_mb']} MB")
        logger.info("=" * 80)

        # SECONDARY COPILOT (Validator) - Final validation
        logger.info("ü§ñ SECONDARY COPILOT VALIDATION:")
        logger.info("‚úÖ Visual processing indicators: COMPLIANT")
        logger.info("‚úÖ Anti-recursion protection: VALIDATED")
        logger.info("‚úÖ Backup integrity: VERIFIED")
        logger.info("‚úÖ Cleanup process: COMPLETED")

        return True

    except Exception as e:
        logger.error(f"‚ùå CLEANUP EXECUTION FAILED: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
