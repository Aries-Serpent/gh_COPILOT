#!/usr/bin/env python3
"""
codex_workflow_analytics_logging.py

Implements the sequential workflow:
- Adds a robust analytics logger wrapper (no auto-migration).
- Instruments major steps (schema compare, diff, reconcile) with analytics logging.
- Parses README to normalize stray placeholders and tracks edits.
- Creates a dedicated pytest validating analytics logging (using a temp DB).
- Captures errors as research questions for ChatGPT-5.
- Writes a structured change log and does NOT touch .github/.

Usage:
  python tools/codex_workflow_analytics_logging.py --execute
"""

from __future__ import annotations
import argparse
import datetime as dt
import logging
import os
import re
import sys
import uuid
from pathlib import Path
from typing import List

# -----------------------------
# Constants / Paths
# -----------------------------
REPO_ROOT = Path(__file__).resolve().parents[1]
UTILS_DIR = REPO_ROOT / "utils"
TOOLS_DIR = REPO_ROOT / "tools"
TESTS_DIR = REPO_ROOT / "tests"
CHANGES_DIR = REPO_ROOT / "codex_changes"
LOGS_DIR = REPO_ROOT / "run_logs"
ERROR_LOG = REPO_ROOT / ".codex_error_log.md"
READMES = [REPO_ROOT / "README.md", REPO_ROOT / "README", REPO_ROOT / "Readme.md"]
# Policy: do not mutate analytics schema automatically
DEFAULT_ANALYTICS_DB = REPO_ROOT / "databases" / "analytics.db"
EVENTS_TABLE_NAME = "events"  # as referenced in README
FALLBACK_FILE_LOG = LOGS_DIR / "analytics_fallback.ndjson"

# Inject into these function name patterns
INJECT_FN_PATTERNS = [
    r"compare[_]?schema",
    r"schema[_]?compare",
    r"compute[_]?diff",
    r"diff[_]?(compute|apply)?",
    r"reconcile",
    r"apply[_]?changes",
]

# Candidate files (heuristic discovery)
CANDIDATE_FILE_HINTS = [
    "cross_database_reconciler.py",
    "database_first_synchronization_engine.py",
    "cross_database_sync_logger.py",
    "database_event_monitor.py",
    "unified_session_management_system.py",
]

# -----------------------------
# Utilities
# -----------------------------
def setup_logger() -> logging.Logger:
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    logger = logging.getLogger("codex_workflow")
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(LOGS_DIR / "codex_workflow_analytics_logging.log", encoding="utf-8")
    fh.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))
    if not logger.handlers:
        logger.addHandler(fh)
    return logger

LOGGER = setup_logger()
RUN_ID = os.environ.get("GH_COPILOT_RUN_ID") or str(uuid.uuid4())

def now_iso() -> str:
    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat()

def safe_rel(p: Path) -> str:
    try:
        return str(p.relative_to(REPO_ROOT))
    except Exception:
        return str(p)

def append_research_question(step: str, err: Exception, context: str) -> None:
    ERROR_LOG.touch(exist_ok=True)
    block = (
        f"\n\n---\n"
        f"**Question for ChatGPT-5:**\n"
        f"While performing [{step}], encountered the following error:\n"
        f"`{type(err).__name__}: {err}`\n"
        f"**Context:** {context}\n"
        f"What are the possible causes, and how can this be resolved while preserving intended functionality?\n"
        f"---\n"
    )
    ERROR_LOG.write_text(ERROR_LOG.read_text(encoding="utf-8") + block, encoding="utf-8")
    LOGGER.error("Research question appended for step %s: %s", step, err)

# -----------------------------
# 2.1 Create analytics logger wrapper
# -----------------------------
ANALYTICS_LOGGER_CODE = '''\
# Auto-generated by tools/codex_workflow_analytics_logging.py
from __future__ import annotations
import json
import os
import sqlite3
import uuid
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional

__all__ = ["log_analytics_event", "get_run_id"]

def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def get_run_id() -> str:
    return os.environ.get("GH_COPILOT_RUN_ID") or str(uuid.uuid4())

def log_analytics_event(
    db_path: str | os.PathLike,
    *,
    level: str,
    step: str,
    phase: str,
    event: str,
    details: dict | None = None,
    run_id: Optional[str] = None,
    error: str | None = None,
    table: str = "events",
    fallback_file: str | os.PathLike | None = None,
) -> bool:
    """
    Policy-compliant analytics logger:
    - Attempts to INSERT into `table` without creating/migrating schemas.
    - On failure (e.g., missing table), writes JSONL fallback and returns False.
    - Always includes a unique run_id and UTC ISO timestamp.
    """
    run_id = run_id or get_run_id()
    payload = {
        "event_time": _now_iso(),
        "level": level,
        "event": event,
        "details": json.dumps({
            "phase": phase,
            "step": step,
            "run_id": run_id,
            "meta": (details or {}),
            "error": error,
        }, ensure_ascii=False),
    }
    try:
        conn = sqlite3.connect(str(db_path))
        try:
            # NOTE: no CREATE TABLE here -> respects repo's "no auto-migration" policy.
            with conn:
                conn.execute(
                    f"INSERT INTO {table}(event_time, level, event, details) VALUES (:event_time, :level, :event, :details)",
                    payload,
                )
        finally:
            conn.close()
        return True
    except Exception as e:
        # Fallback to JSONL file if DB insert fails
        record = {
            "fallback_time": _now_iso(),
            "reason": f"{type(e).__name__}: {e}",
            "attempt": payload,
        }
        if fallback_file:
            Path(fallback_file).parent.mkdir(parents=True, exist_ok=True)
            with open(fallback_file, "a", encoding="utf-8") as fh:
                fh.write(json.dumps(record, ensure_ascii=False) + "\n")
        return False
'''

def ensure_analytics_logger() -> Path:
    UTILS_DIR.mkdir(parents=True, exist_ok=True)
    target = UTILS_DIR / "analytics_logger.py"
    if not target.exists():
        target.write_text(ANALYTICS_LOGGER_CODE, encoding="utf-8")
        LOGGER.info("Created %s", safe_rel(target))
    return target

# -----------------------------
# 2.2 README parser (light, non-destructive)
# -----------------------------
def normalize_readme(changelog: List[str]) -> None:
    for rd in READMES:
        if rd.exists():
            original = rd.read_text(encoding="utf-8")
            fixed = re.sub(r"\(\)", "", original)  # remove stray "()" placeholders
            if fixed != original:
                rd.write_text(fixed, encoding="utf-8")
                changelog.append(f"Normalized placeholders in {safe_rel(rd)} (removed stray '()').")
            return

# -----------------------------
# 2.3 Instrumentation helpers
# -----------------------------
INJECT_IMPORT_LINE = "from utils.analytics_logger import log_analytics_event\n"
INJECT_CALL_TEMPLATE = (
    '    try:\n'
    '        log_analytics_event("{db}", level="INFO", step="{step}", phase="{phase}", event="{event}", details={{"file": "{file}", "function": "{fn}"}}, fallback_file="{fallback}")\n'
    '    except Exception as _e:\n'
    '        # Swallow logging errors to keep business logic unaffected\n'
    '        pass\n'
)

def find_candidate_files() -> List[Path]:
    hits = set()
    for hint in CANDIDATE_FILE_HINTS:
        for p in REPO_ROOT.rglob(hint):
            hits.add(p)
    # Add broader scan to catch modules with likely functions
    for p in REPO_ROOT.rglob("*.py"):
        try:
            txt = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        if re.search("|".join(INJECT_FN_PATTERNS), txt):
            hits.add(p)
    return sorted(hits)

def inject_logging_calls(pyfile: Path, changelog: List[str], analytics_db: Path, fallback_file: Path) -> None:
    try:
        src = pyfile.read_text(encoding="utf-8")
    except Exception as e:
        append_research_question("2.3:load_file", e, f"File={safe_rel(pyfile)}")
        return

    updated = src
    # Ensure import exists once
    if "log_analytics_event" not in src and INJECT_IMPORT_LINE.strip() not in src:
        # Add after first import block or at top
        updated = INJECT_IMPORT_LINE + updated

    # Inject in function bodies cautiously
    injected_count = 0
    def repl(m):
        nonlocal injected_count
        fn_head = m.group(0)
        fn_name = m.group("name")
        call = INJECT_CALL_TEMPLATE.format(
            db=str(analytics_db).replace("\\", "/"),
            step=f"{fn_name}",
            phase="schema_diff_reconcile",
            event="enter_function",
            file=safe_rel(pyfile),
            fn=fn_name,
            fallback=str(fallback_file).replace("\\", "/"),
        )
        injected_count += 1
        return fn_head + call

    pattern = re.compile(
        r"^def\s+(?P<name>(" + "|".join(INJECT_FN_PATTERNS) + r"))\s*\(",
        flags=re.MULTILINE,
    )
    updated2 = pattern.sub(repl, updated)

    if updated2 != src:
        pyfile.write_text(updated2, encoding="utf-8")
        changelog.append(f"Injected {injected_count} logger call(s) into {safe_rel(pyfile)}")
    elif injected_count == 0:
        # no change; not an error, but we record a note
        changelog.append(f"No eligible functions found in {safe_rel(pyfile)} (skipped).")

# -----------------------------
# 2.4 Create dedicated tests
# -----------------------------
TEST_CODE = '''\
import os
import sqlite3
import tempfile
from pathlib import Path

from utils.analytics_logger import log_analytics_event, get_run_id

def _prepare_temp_db():
    # Use a temp db to avoid touching the repo's analytics.db (policy: no auto-migration).
    tmp = tempfile.NamedTemporaryFile(suffix=".db", delete=False)
    tmp.close()
    conn = sqlite3.connect(tmp.name)
    with conn:
        conn.execute("CREATE TABLE events (event_time TEXT, level TEXT, event TEXT, details TEXT)")
    conn.close()
    return Path(tmp.name)

def test_log_analytics_event_inserts_row():
    db = _prepare_temp_db()
    try:
        ok = log_analytics_event(
            db,
            level="INFO",
            step="compare_schema",
            phase="schema_diff_reconcile",
            event="enter_function",
            details={"unit_test": True},
            run_id=get_run_id(),
        )
        assert ok, "Expected insert to succeed"

        conn = sqlite3.connect(db)
        rows = list(conn.execute("SELECT event_time, level, event, details FROM events"))
        conn.close()
        assert len(rows) == 1
        assert rows[0][1] == "INFO"
        assert rows[0][2] == "enter_function"
    finally:
        if db.exists():
            db.unlink()
'''

def ensure_test_file() -> Path:
    TESTS_DIR.mkdir(parents=True, exist_ok=True)
    t = TESTS_DIR / "test_analytics_logger.py"
    if not t.exists():
        t.write_text(TEST_CODE, encoding="utf-8")
        LOGGER.info("Created %s", safe_rel(t))
    return t

# -----------------------------
# Change log writer
# -----------------------------
def write_changelog(changes: List[str]) -> Path:
    CHANGES_DIR.mkdir(parents=True, exist_ok=True)
    p = CHANGES_DIR / "CHANGELOG_ANALYTICS_LOGGING.md"
    ts = now_iso()
    body = [f"## {ts} â€“ Analytics Logging Enhancements (RUN_ID={RUN_ID})"]
    if not changes:
        body.append("- No code changes were needed.")
    else:
        body += [f"- {c}" for c in changes]
    if p.exists():
        p.write_text(p.read_text(encoding="utf-8") + "\n\n" + "\n".join(body) + "\n", encoding="utf-8")
    else:
        p.write_text("\n".join(body) + "\n", encoding="utf-8")
    return p

# -----------------------------
# Coverage Performance printer
# -----------------------------
def print_coverage_performance():
    """
    This does not run tests; it prints the formula and instructions
    to compute \hat{C} once tests/coverage run in CI or locally.
    """
    formula = (
        "C_hat = 0.5 * (tests_passed/tests_total)"
        " + 0.3 * (lines_covered/lines_total)"
        " + 0.2 * (logging_assertions_ok/logging_assertions_total)"
    )
    print("[Coverage Performance] Composite formula:", formula)
    print("[Coverage Performance] Run your suite then compute metrics to produce C_hat.")

# -----------------------------
# Orchestration
# -----------------------------
def main(execute: bool = False) -> int:
    changes: List[str] = []
    # 2.1
    try:
        analytics_logger_py = ensure_analytics_logger()
        changes.append(f"Added/confirmed {safe_rel(analytics_logger_py)}")
    except Exception as e:
        append_research_question("2.1:create_analytics_logger", e, "Writing utils/analytics_logger.py")
    # 2.2
    try:
        normalize_readme(changes)
    except Exception as e:
        append_research_question("2.2:normalize_readme", e, "Non-destructive placeholder cleanup")
    # 2.3
    if execute:
        try:
            files = find_candidate_files()
            for f in files:
                inject_logging_calls(
                    f, changes, analytics_db=DEFAULT_ANALYTICS_DB, fallback_file=FALLBACK_FILE_LOG
                )
        except Exception as e:
            append_research_question("2.3:instrumentation", e, "Scanning & injecting log calls")
    # 2.4
    try:
        test_py = ensure_test_file()
        changes.append(f"Added/confirmed {safe_rel(test_py)}")
    except Exception as e:
        append_research_question("2.4:add_test", e, "Creating tests/test_analytics_logger.py")

    # 5.x Finalization
    chg = write_changelog(changes)
    print(f"[OK] Wrote change log: {safe_rel(chg)}")
    print_coverage_performance()
    print(f"[NOTE] RUN_ID={RUN_ID}")
    print("[SAFE] No GitHub Actions were touched.")
    return 0

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--execute", action="store_true", help="Perform code instrumentation (safe & idempotent).")
    args = ap.parse_args()
    sys.exit(main(execute=args.execute))
